<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="author" content="Pybonacci">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">
        <title>Introducci√≥n a Machine Learning con Python (Parte 1) | Pybonacci</title>

	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/favicon.ico" type="image/x-icon">
        <link rel="alternate" type="application/atom+xml" title="Pybonacci blog atom feed" href="/feeds/all.atom.xml" />
        <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700' rel='stylesheet' type='text/css'>

        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link rel="stylesheet" type="text/css" href="/theme/css/fontello.css"/>
        <style>.highlight .hll { background-color: #ffffcc }
.highlight .c { color: #60a0b0; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #007020; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .cm { color: #60a0b0; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #007020 } /* Comment.Preproc */
.highlight .c1 { color: #60a0b0; font-style: italic } /* Comment.Single */
.highlight .cs { color: #60a0b0; background-color: #fff0f0 } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #808080 } /* Generic.Output */
.highlight .gp { color: #c65d09; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0040D0 } /* Generic.Traceback */
.highlight .kc { color: #007020; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #007020; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #007020; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #007020 } /* Keyword.Pseudo */
.highlight .kr { color: #007020; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #902000 } /* Keyword.Type */
.highlight .m { color: #40a070 } /* Literal.Number */
.highlight .s { color: #4070a0 } /* Literal.String */
.highlight .na { color: #4070a0 } /* Name.Attribute */
.highlight .nb { color: #007020 } /* Name.Builtin */
.highlight .nc { color: #0e84b5; font-weight: bold } /* Name.Class */
.highlight .no { color: #60add5 } /* Name.Constant */
.highlight .nd { color: #555555; font-weight: bold } /* Name.Decorator */
.highlight .ni { color: #d55537; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #007020 } /* Name.Exception */
.highlight .nf { color: #06287e } /* Name.Function */
.highlight .nl { color: #002070; font-weight: bold } /* Name.Label */
.highlight .nn { color: #0e84b5; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #062873; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #bb60d5 } /* Name.Variable */
.highlight .ow { color: #007020; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mf { color: #40a070 } /* Literal.Number.Float */
.highlight .mh { color: #40a070 } /* Literal.Number.Hex */
.highlight .mi { color: #40a070 } /* Literal.Number.Integer */
.highlight .mo { color: #40a070 } /* Literal.Number.Oct */
.highlight .sb { color: #4070a0 } /* Literal.String.Backtick */
.highlight .sc { color: #4070a0 } /* Literal.String.Char */
.highlight .sd { color: #4070a0; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #4070a0 } /* Literal.String.Double */
.highlight .se { color: #4070a0; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #4070a0 } /* Literal.String.Heredoc */
.highlight .si { color: #70a0d0; font-style: italic } /* Literal.String.Interpol */
.highlight .sx { color: #c65d09 } /* Literal.String.Other */
.highlight .sr { color: #235388 } /* Literal.String.Regex */
.highlight .s1 { color: #4070a0 } /* Literal.String.Single */
.highlight .ss { color: #517918 } /* Literal.String.Symbol */
.highlight .bp { color: #007020 } /* Name.Builtin.Pseudo */
.highlight .vc { color: #bb60d5 } /* Name.Variable.Class */
.highlight .vg { color: #bb60d5 } /* Name.Variable.Global */
.highlight .vi { color: #bb60d5 } /* Name.Variable.Instance */
.highlight .il { color: #40a070 } /* Literal.Number.Integer.Long */</style>
        <style>.description-author {
  font-size: 0.8em;
  color: #333;
}
.img-author {
  max-height: 10em;
  max-width: 10em;
}
img[src$="centerme"] {
  display: block;
  margin: 0 auto;
}
body {
  margin: 0;
  padding: 0;
  font: 15px 'Source Sans Pro', sans-serif;
  line-height: 1.6em;
  color: #222222;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
}
a {
  color: #007ee5;
  text-decoration: none;
}
a:hover {
  color: #007ee5;
  text-decoration: none;
}
header.main-header {
  background: none repeat scroll 0% 0% #205F29;
  margin-bottom: 0px;
}
header.main-header a {
  color: #fff;
}
header.main-header .container {
  max-width: 1000px;
}
header.main-header .container nav a:hover {
  background-color: #5C881C;
}
header.navbar-default {
  border-bottom: none;
  background-color: #EFEFEF;
}
article {
  margin: 0;
}
article header.about {
  margin-bottom: 0px;
  padding-bottom: 0px;
}
article header {
  padding-bottom: 20px;
}
article header h1 {
  margin-bottom: 2px;
  font-weight: 700;
  color: #000;
}
article header time {
  color: #9E9E9E;
  float: right;
}
article header time.left {
  color: #9E9E9E;
  float: left;
}
article div.social-links ul {
  padding: 0px;
}
article div.social-links li {
  display: inline;
  font-size: 20px;
}
article div.social-links li a {
  color: #000;
  padding: 10px;
}
article div.social-links li a:hover {
  color: #666;
  text-decoration: none;
}
article p {
  font-size: 2em;
  margin-bottom: 20px;
  line-height: 1.6em;
  text-align: justify;
}
article p.note {
  background: #f5f5f5;
  border: 1px solid #ddd;
  padding: 0.533em 0.733em;
}
article p.update {
  background-color: #FEEFB3;
  border: 1px solid #e6e68a;
  padding: 0.533em 0.733em;
}
article p.alert {
  background-color: #ffe2e2;
  border: 1px solid #ffb2b2;
  padding: 0.533em 0.733em;
}
article ul,
article ol {
  margin-top: 0px;
  margin-bottom: 25px;
}
article li {
  font-size: 16px;
  line-height: 1.6em;
}
article a:hover {
  text-decoration: underline;
}
article blockquote {
  border-left: 2px solid #c7c7cc;
  color: #666;
  margin: 30px 0;
  padding: 0 0 0 25px;
}
article img {
  max-width: 100%;
}
article code {
  color: #333;
  background-color: #EEE;
  border-radius: 0;
  font-size: 1.1em;
}
article .meta {
  margin-top: 35px;
}
article .meta a:hover {
  text-decoration: none;
}
article .meta div {
  display: block;
}
article .meta address:before,
article .meta time:before,
article .meta a.tag:before {
  font-family: 'fontello';
  margin-right: 6px;
}
article .meta address.author {
  float: left;
}
article .meta address:before {
  content: '\e819';
}
article .meta time:before {
  content: '\f133';
}
article .meta div.tags {
  clear: both;
}
article .meta a.tag {
  margin: 0 10px 10px 0;
  padding: 1px 12px;
  display: inline-block;
  font-size: 14px;
  color: rgba(0, 0, 0, 0.8);
  background: rgba(0, 0, 0, 0.05);
}
article .meta a.tag:before {
  content: '\e821';
}
article .meta a.tag:hover {
  background: rgba(0, 0, 0, 0.15);
}
article .meta a.read_more,
article .meta a.comments_btn {
  font-size: 14px;
  font-weight: 800;
  padding: 10px 20px;
  color: #007aa3;
  background: #FFF;
  border: 1px solid #007aa3;
}
article .meta a.read_more:hover,
article .meta a.comments_btn:hover {
  color: #FFF;
  background: #007aa3;
}
article .meta:after {
  content: "";
  display: table;
  clear: both;
}
.index {
  max-width: 700px;
}
.index article header h2 {
  font-size: 36px;
  margin-bottom: 2px;
  font-weight: 700;
}
.index article header h2 a {
  color: #333;
}
.index article header h2 a:hover {
  color: #007ee5;
  text-decoration: none;
}
.index .separator {
  padding: 40px 0 0 0;
  margin: 0 0 40px 0;
  height: 10px;
  border-bottom: solid 1px #CCC;
}
.index .pagination {
  display: block;
  margin-bottom: 100px;
}
.index .pagination .left {
  text-align: right;
}
.index .pagination .right {
  text-align: left;
}
.index .pagination a {
  display: inline-block;
  border: 2px solid #5C881C;
  margin: 0 5px;
  padding: 8px 20px;
  font-weight: bold;
  color: #5C881C;
}
.index .pagination a:hover {
  color: #FFF;
  background: #5C881C;
}
.post {
  max-width: 80%;
}
.post h1 {
  font-size: 42px;
}
.post h2:before {
  content: "# ";
  font-weight: bold;
  color: #DDD;
}
.post h3:before {
  content: "## ";
  font-weight: bold;
  color: #DDD;
}
.post h4:before {
  content: "### ";
  font-weight: bold;
  color: #DDD;
}
.post p {
  font-size: 2em;
}
.post li {
  font-size: 2em;
}
.list {
  max-width: 700px;
}
.list ul.double-list {
  margin: 0 auto 60px;
  padding: 0;
  list-style-type: none;
}
.list ul.double-list li {
  padding: 5px 0;
}
.list ul.double-list li h2 {
  font-size: 2em;
  display: inline;
  font-weight: normal;
}
.list ul.double-list li span {
  font-family: sans-serif;
  text-transform: uppercase;
  text-align: right;
  float: right;
  padding-top: 3px;
  font-size: 12px;
  color: #999;
}
.full-width-content {
  padding-top: 10px;
  padding-left: 0px;
  padding-right: 0px;
  margin-left: -20px;
  margin-right: -20px;
}
.col-xs-1,
.col-sm-1,
.col-md-1,
.col-lg-1,
.col-xs-2,
.col-sm-2,
.col-md-2,
.col-lg-2,
.col-xs-3,
.col-sm-3,
.col-md-3,
.col-lg-3,
.col-xs-4,
.col-sm-4,
.col-md-4,
.col-lg-4,
.col-xs-5,
.col-sm-5,
.col-md-5,
.col-lg-5,
.col-xs-6,
.col-sm-6,
.col-md-6,
.col-lg-6,
.col-xs-7,
.col-sm-7,
.col-md-7,
.col-lg-7,
.col-xs-8,
.col-sm-8,
.col-md-8,
.col-lg-8,
.col-xs-9,
.col-sm-9,
.col-md-9,
.col-lg-9,
.col-xs-10,
.col-sm-10,
.col-md-10,
.col-lg-10,
.col-xs-11,
.col-sm-11,
.col-md-11,
.col-lg-11,
.col-xs-12,
.col-sm-12,
.col-md-12,
.col-lg-12 {
  padding-right: 0px;
  padding-left: 0px;
}
.disclaimer {
  text-align: center;
  background-color: #EFEFEF;
  border-bottom: none;
  margin-top: 6em;
}
pre {
  font-size: 1.2em;
}</style>

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>

       <script src="https://comments.pybonacci.org/assets/js/commento.js" data-div="#commento"></script>

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        });
        MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
            }
        });
    </script>

    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>


        </script>

    </head>

    <body>
        <header class="navbar navbar-default bs-docs-nav">
            <div class="container-fluid">
                <div class="navbar-header">
		  <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#theNavbar">
		    <span class="icon-bar"></span>
		    <span class="icon-bar"></span>
		    <span class="icon-bar"></span> 
		  </button>
                  <a class="navbar-brand" href="/" title="Home" class="title">Pybonacci</a><!-- ‚Äî Python y Ciencia-->
                </div>
                <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation" id="theNavbar">
		    <ul class="nav navbar-nav navbar-right">
                            <li><a href="/pages/acerca-de-pybonacci.html" title="About">Acerca de</a></li>
                            <li><a href="/archives.html" title="Archive">Archivos</a></li>
                            <li><a class="nodec icon-rss" href="/feeds/all.atom.xml" title="pybonacci.github.io RSS feed" rel="me"></a></li>
                    </ul>
                </nav>
            </div>
        </header>

        <div id="wrap">
<div class="container post">
    <article>
        <header>
            <h1>Introducci√≥n a Machine Learning con Python (Parte 1)</h1>
            <div class="meta">
                <time datetime="article.date.isoformat()" pubdate>mi√© 14 enero 2015</time>
                <address class="vcard author">Por
                    <a class="url fn" href="http://pybonacci.github.io/author/pablo-fernandez.html">Pablo Fern√°ndez</a>
                </address>
            </div>
        </header>

        <div class="article_content">
            <p>Desde que escuch√© hablar de Kaggle por primera vez, precisamente <a href="https://twitter.com/Pybonacci/status/414742882679934977" target="_blank">a trav√©s de Pybonacci</a>, me entr√≥ curiosidad por eso del <em>data science</em> y me propuse como un reto el participar en una de sus competiciones. Para aquel que no la conozca todav√≠a,¬†<a title="Kaggle" href="http://www.kaggle.com/" target="_blank">Kaggle</a> es una plataforma que aloja competiciones de an√°lisis de datos y modelado predictivo donde compa√±√≠as e investigadores aportan sus datos mientras que estadistas e ingenieros de datos de todo el mundo compiten por crear los mejores modelos de predicci√≥n o clasificaci√≥n.</p>
<p>Muchas y muy diferentes t√©cnicas se pueden aplicar al procesado de datos para generar predicciones, estimaciones o clasificaciones. Desde t√©cnicas de <a href="http://es.wikipedia.org/wiki/Regresi√≥n_log√≠stica" target="_blank">regresi√≥n log√≠stica</a> hasta <a href="http://es.wikipedia.org/wiki/Red_neuronal_artificial" target="_blank">redes neuronales artificiales</a> pasando por <a href="http://es.wikipedia.org/wiki/Red_bayesiana" target="_blank">redes bayesianas</a>, <a href="http://es.wikipedia.org/wiki/M√°quinas_de_vectores_de_soporte" target="_blank">m√°quinas de vectores de soporte</a> o <a href="http://es.wikipedia.org/wiki/√Årbol_de_decisi√≥n" target="_blank">√°rboles de decisi√≥n</a>, en Kaggle no descartan ning√∫n m√©todo, e incluso se fomenta la cooperaci√≥n entre personas con experiencia en diferentes campos para obtener el mejor modelo posible. Varias de estas t√©cnicas se encuadran dentro de lo que es el Machine Learning, o aprendizaje autom√°tico, que nos explica <a href="http://en.wikipedia.org/wiki/Jeremy_Howard_(entrepreneur)" target="_blank">Jeremy Howard</a> en el siguiente v√≠deo.</p>
<!--more Sigue leyendo... >-->

<p>En resumen, el objetivo del machine learning (ML) es ense√±ar a las m√°quinas el llevar a cabo ciertas tareas ense√±√°ndoles algunos ejemplos de c√≥mo o c√≥mo no llevar a cabo la tarea. Esto rara vez es un proceso en cascada y, en multitud de ocasiones, habr√° que retroceder varios pasos para probar diferentes estrategias sobre el conjunto de datos con diferentes algoritmos ML. En palabras de Richert y Coelho (2013), es √©ste car√°cter exploratorio lo que se ajusta a la perfecci√≥n a Python.</p>
<blockquote>
<p>Being an interpreted high-level programming language, it may seem that Python was designed specifically for the process of trying out different things. What is more, it does this very fast.</p>
</blockquote>
<p>Visto el potencial de Python en este campo, la comunidad de desarrolladores ha aportado varios paquetes como <a href="http://pybrain.org/pages/home" target="_blank">PyBrain</a> (Schaul et al., 2010) o <a href="http://scikit-learn.org/stable/index.html" target="_blank">scikit-learn</a> (Pedregosa et al., 2011) entre otros al campo del aprendizaje autom√°tico. De todos ellos, el m√°s conocido tal vez sea scikit-learn, y es el que utilizaremos m√°s a menudo para ilustrar los ejemplos.</p>
<p>Pero antes de meternos de lleno al aprendizaje autom√°tico vamos a ver unos ejercicios de clasificaci√≥n b√°sicos que nos permitir√°n entender mejor la posibilidades y limitaciones de algoritmos mucho m√°s complejos.</p>
<h2>El <em>Iris flower dataset</em></h2>
<p>El conjunto de datos de la planta Iris data de los a√±os '30 y es empleado con frecuencia como ejemplo por diferentes librer√≠as que trabajan con datos o gr√°ficos¬†como <a href="http://pandas.pydata.org/pandas-docs/stable/visualization.html#andrews-curves" target="_blank">pandas</a>¬†o el propio <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" target="_blank">scikit-learn</a>. De¬†cada planta de la especie Iris (setosa, versicolor y virginica) se han tomado medidas de longitud y ancho de s√©palo y p√©talo. Y la pregunta que se suele plantear es: <em>si vemos una nueva planta en el campo, ¬øpodr√≠amos predecir correctamente su especie a partir de sus medidas?</em></p>
<p>Este es un problema de aprendizaje supervisado o clasificaci√≥n. Puesto que el conjunto de datos es peque√±o, hemos representado las proyecciones bidimensionales como subgr√°ficos de un solo gr√°fico con el que podemos identificar dos grandes grupos: uno formado por Iris Setosa y otro formado por una mezcla de Iris Versicolor e Iris Virginica.</p>
<h3>Primer¬†modelo de clasificaci√≥n</h3>
<p>Nuestro primer modelo de clasificaci√≥n se basar√° precisamente en esa primera agrupaci√≥n visual que hemos realizado. Es decir, si la longitud del p√©talo es inferior a 2, entonces se trata de Iris Setosa, si no, puede ser Iris Versicolor o Iris Virginica.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">read_csv</span>
<span class="c1"># leemos el dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;iris.csv&#39;</span><span class="p">)</span>
<span class="c1"># Separamos Iris Setosa de las otras dos especies en funci√≥n de la longitud</span>
<span class="c1"># del p√©talo.</span>
<span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">iris</span><span class="o">.</span><span class="n">PetalLength</span><span class="o">.</span><span class="n">values</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">value</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Iris setosa&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Iris virginica o Iris versicolor&#39;</span><span class="p">)</span>
</pre></div>


<p>Lo que hemos creado es un simple umbral en una de las dimensiones. Lo hemos hecho de manera visual; el aprendizaje autom√°tico tiene lugar cuando escribimos c√≥digo que realiza esto mismo por nosotros.</p>
<p>Distinguir Iris Setosa de las otras dos especies fue sencillo. Sin embargo, no tenemos forma de ver inmediatamente cual es el mejor umbral para distinguir Iris Virginica de Iris Versicolor. Es m√°s, podemos deducir que la distinci√≥n nunca ser√° perfecta. Pero podemos generar un algoritmo sencillo que nos de la mejor soluci√≥n de compromiso en base a los par√°metros medidos de las plantas.</p>
<p>En el siguiente fragmento de c√≥digo vamos a buscar, de entre las cuatro caracter√≠sticas medidas ‚Äîlongitud y ancho de s√©palo y p√©talo‚Äî, el valor de umbral que mejor clasifica la familia de Iris.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">read_csv</span>
<span class="c1"># leemos el dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;iris.csv&#39;</span><span class="p">)</span>
<span class="c1"># descartamos la familia setosa que ya tenemos clasificada</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="n">iris</span><span class="o">.</span><span class="n">Name</span> <span class="o">!=</span> <span class="s1">&#39;Iris-setosa&#39;</span><span class="p">]</span>
<span class="n">virginica</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">Name</span><span class="o">==</span><span class="s1">&#39;Iris-virginica&#39;</span>
<span class="c1"># obtenemos un array con los nombres de las caracter√≠sticas que medimos</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span>
<span class="c1"># inicializamos en valor de precisi√≥n</span>
<span class="n">best_acc</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="k">for</span> <span class="n">fi</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>                    <span class="c1"># Por cada par√°metro o caracter√≠stica de la que tenemos valores</span>
    <span class="n">thresh</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="n">fi</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>           <span class="c1"># obtenemos una lista de valores para el umbral</span>
    <span class="n">thresh</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>          <span class="c1"># que ordenamos de menor a mayor.</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">thresh</span><span class="p">:</span>                   <span class="c1"># Por cada posible valor de umbral</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">iris</span><span class="p">[</span><span class="n">fi</span><span class="p">]</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="n">t</span><span class="p">)</span>       <span class="c1"># determinamos los elementos de la tabla que est√°n por encima</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred</span><span class="o">==</span><span class="n">virginica</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># y calculamos que porcentaje de la familia virginica est√° recogida.</span>
        <span class="k">if</span> <span class="n">acc</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="n">best_acc</span><span class="p">:</span>          <span class="c1"># Si mejoramos la detecci√≥n, actualizamos los par√°metro de la colecci√≥n.</span>
            <span class="n">best_acc</span> <span class="o">=</span> <span class="n">acc</span>             <span class="c1"># Mejor precisi√≥n obtenida.</span>
            <span class="n">best_fi</span> <span class="o">=</span> <span class="n">fi</span>               <span class="c1"># Mejor caracter√≠stica para clasificar las familias.</span>
            <span class="n">best_t</span> <span class="o">=</span> <span class="n">t</span>                 <span class="c1"># Valor √≥ptimo de umbral.</span>
</pre></div>


<p>Seg√∫n el algoritmo que hemos implementado, el valor √≥ptimo de umbral es de 1.6 cm de ancho de p√©talo. Con ese valor, clasificamos correctamente el 94% de las plantas como Virginica. En este tipo de modelos de umbral, la frontera de decisi√≥n ser√° siempre paralela a uno de los ejes.</p>
<h3>Validaci√≥n cruzada</h3>
<p>El modelo, a pesar de su simplicidad, logra un 94% de acierto sobre los datos de entrenamiento. No obstante, se trata de una valoraci√≥n bastante optimista pues empleamos los propios datos de entrenamiento para evaluar el modelo. Lo que realmente queremos es estimar la habilidad del modelo de generalizar en nuevos casos.</p>
<p>Para determinar las capacidades del modelo, u obtener un modelo m√°s robusto, se suele recurrir a la <a href="http://es.wikipedia.org/wiki/Validaci√≥n_cruzada" target="_blank">validaci√≥n cruzada</a>. De esta manera utilizamos parte de los datos de que disponemos para entrenar el modelo ‚Äî<em>training set</em>‚Äî, y el resto de datos para probarlo ‚Äî<em>test set</em>‚Äî (ver imagen inferior).<figure style="width: 526px" class="wp-caption aligncenter"></p>
<p><img src="http://upload.wikimedia.org/wikipedia/commons/f/f2/K-fold_cross_validation.jpg" width="526" height="262" class /><figcaption class="wp-caption-text">Validaci√≥n cruzada de k=4 iteraciones [<a href="http://es.wikipedia.org/wiki/Validaci%C3%B3n_cruzada#Error_de_la_validaci.C3.B3n_cruzada_de_K_iteraciones" target="_blank">Fuente</a>].</figcaption></figure> </p>
<h3>K nearest neighbors</h3>
<p>Un paso m√°s hacia lo que ser√≠a <em>machine learning</em> es el m√©todo <a href="http://es.wikipedia.org/wiki/Knn" target="_blank"><em>k</em>-nn</a> de clasificaci√≥n supervisada. En este caso, a la hora de clasificar un nuevo elemento, buscamos en el conjunto de datos de que disponemos al punto, o <em>k</em>-puntos m√°s cercanos, y le asignamos su catogor√≠a.</p>
<p>Si bien podemos <a href="http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/" target="_blank">implementar el algoritmo a mano en Python</a> ‚Äînunca est√° de m√°s saber c√≥mo funcionan las cosas‚Äî, scikit-learn incluye la herramienta <a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" target="_blank">KNeighborsClassifier</a> que ya realiza todo el trabajo pesado por nosotros. Aqu√≠ voy a adaptar ligeramente el <a href="http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#example-neighbors-plot-classification-py" target="_blank">ejemplo</a> que proporciona scikit-learn para utilizarlo con pandas y generar unos gr√°ficos diferentes que nos permitan compararlo con el modelo anterior.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
</pre></div>


<p>Aunque scikit-learn incluye su propio dataset de Iris, vamos a importar el mismo que hemos empleado en los casos anteriores.</p>
<div class="highlight"><pre><span></span><span class="n">iris</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;iris.csv&#39;</span><span class="p">)</span>          <span class="c1"># Importamos el dataset.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[[</span><span class="s1">&#39;PetalLenght&#39;</span><span class="p">,</span> <span class="s1">&#39;PetalWidth&#39;</span><span class="p">]]</span> <span class="c1"># Tomamos el ancho y longitud del p√©talo.</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="s1">&#39;Name&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">)</span>     <span class="c1"># Para crear luego los gr√°ficos vamos a necesitar categorizar los nombres</span>
<span class="n">y</span><span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>            <span class="c1"># y asignarles un n√∫mero a cada variedad de planta; en este caso tres.</span>
</pre></div>


<p>Tomaremos un n√∫mero de vecinos relativamente grande, <em>k</em> = 15. El valor de <em>k</em> depende mucho de los datos de que dispongamos, pero en general, un valor alto mitiga el ruido a costa de diferenciar menos zonas. Definimos tambi√©n el espesor de la malla y los <em>colormaps</em> del gr√°fico.</p>
<div class="highlight"><pre><span></span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">h</span> <span class="o">=</span> <span class="o">.</span><span class="mo">02</span>          <span class="c1"># step size de la malla</span>
<span class="c1"># Creamos los colormap</span>
<span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FFAAAA&#39;</span><span class="p">,</span> <span class="s1">&#39;#AAFFAA&#39;</span><span class="p">,</span> <span class="s1">&#39;#AAAAFF&#39;</span><span class="p">])</span>
<span class="n">cmap_bold</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="s1">&#39;#00FF00&#39;</span><span class="p">,</span> <span class="s1">&#39;#0000FF&#39;</span><span class="p">])</span>
</pre></div>


<p>Por √∫ltimo, nos metemos de lleno al modelo. √âste, en com√∫n a la mayor√≠a de modelos en scikit-learn, dispone de una serie de funciones que se ejecutan paso a paso.</p>
<ul>
<li><em>nombre-del-modelo</em>.<strong>fit()</strong></li>
<li><em>nombre-del-modelo</em>.<strong>predict()</strong></li>
<li><em>nombre-del-modelo</em>.<strong>score()</strong></li>
</ul>
<p>Con la funci√≥n <code>fit()</code> entrenamos el modelo para obtener los par√°metros que utilizaremos sobre los datos de test con la funci√≥n <code>predict()</code>. Finalmente, con <code>score()</code> podremos obtener una estimaci√≥n de la capacidad de acierto de nuestro modelo sobre los datos de trabajo.</p>
<div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">weights</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="s1">&#39;distance&#39;</span><span class="p">]:</span>
    <span class="c1"># Creamos una instancia de Neighbors Classifier y hacemos un fit a partir de los</span>
    <span class="c1"># datos.</span>
    <span class="c1"># Los pesos (weights) determinar√°n en qu√© proporci√≥n participa cada punto en la</span>
    <span class="c1"># asignaci√≥n del espacio. De manera uniforme o proporcional a la distancia.</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># Creamos una gr√°fica con las zonas asignadas a cada categor√≠a seg√∫n el modelo</span>
    <span class="c1"># k-nearest neighborgs. Para ello empleamos el meshgrid de Numpy.</span>
    <span class="c1"># A cada punto del grid o malla le asignamos una categor√≠a seg√∫n el modelo knn.</span>
    <span class="c1"># La funci√≥n c_() de Numpy, concatena columnas.</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
    <span class="c1"># Ponemos el resultado en un gr√°fico.</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">)</span>
    <span class="c1"># Representamos tambi√©n los datos de entrenamiento.</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">yy</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;3-Class classification (k = </span><span class="si">%i</span><span class="s2">, weights = &#39;</span><span class="si">%s</span><span class="s2">&#39;)&quot;</span>
              <span class="o">%</span> <span class="p">(</span><span class="n">n_neighbors</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Petal Width&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Petal Length&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;iris-knn-{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>
</pre></div>


<p>Las figuras que obtenemos son las siguientes.</p>
<div id='gallery-1' class='gallery galleryid-2976 gallery-columns-2 gallery-size-thumbnail'>
  <figure class='gallery-item'>

  <div class='gallery-icon landscape'>
    <a href='https://pybonacci.org/wp-content/uploads/2015/01/iris-knn-distance.png'><img width="150" height="150" src="https://pybonacci.org/wp-content/uploads/2015/01/iris-knn-distance-150x150.png" class="attachment-thumbnail size-thumbnail" alt="" aria-describedby="gallery-1-3078" /></a>
  </div><figcaption class='wp-caption-text gallery-caption' id='gallery-1-3078'> 3-Class classification (k = 15, weights=&#8217;distance&#8217;) </figcaption></figure><figure class='gallery-item'> 

  <div class='gallery-icon landscape'>
    <a href='https://pybonacci.org/wp-content/uploads/2015/01/iris-knn-uniform.png'><img width="150" height="150" src="https://pybonacci.org/wp-content/uploads/2015/01/iris-knn-uniform-150x150.png" class="attachment-thumbnail size-thumbnail" alt="" aria-describedby="gallery-1-3077" /></a>
  </div><figcaption class='wp-caption-text gallery-caption' id='gallery-1-3077'> 3-Class classification (k = 15, weights=&#8217;uniform&#8217;) </figcaption></figure>
</div>

<p>Como podemos ver, en este caso, las lineas que separan las tres categor√≠as de planta Iris ya no son verticales. El modelo <em>k</em> Nearest Neighborgs que hemos empleado ha considerado que la separaci√≥n de categor√≠as (para selecci√≥n de ordenadas y abscisas) es m√°s bien tirando a horizontal. Visualmente tambi√©n podemos apreciar que √©ste modelo incluye m√°s puntos dentro de la categor√≠a correcta que el modelo simple que hemos estudiado en primer lugar. Si recurrimos a la funci√≥n <code>score()</code>, obtendremos una puntuaci√≥n del 96% en el caso de <code>weights='uniform'</code>, y del 98.6% si optamos por <code>weights='distancia'</code>. Cabe recordar que en este caso tampoco se ha recurrido a un <em>cross validation</em> para puntuar el modelo, por lo que esta puntuaci√≥n puede ser algo optimista.</p>
<h2>Conclusi√≥n</h2>
<p>Espero que a esta primera parte le siga al menos una segunda, donde si que espero poder ense√±ar algo de Machine Learning de verdad. No soy ni mucho menos un aficionado en esto del ML, pues hace bien poco que he empezado, pero como es algo por lo que siempre he sentido un cierto inter√©s, creo que merece la pena el que me embarque en ense√±ar lo que voy aprendiendo; y ver si as√≠ despierto el inter√©s de m√°s gente por el tema, perderle el miedo y no verlo como algo lejano y muy complicado.</p>
<p>Por ahora no ha sido m√°s que una introducci√≥n y pr√°cticamente no se ha tocado nada de aprendizaje autom√°tico, pero hemos tratado algunos conceptos b√°sicos que son clave asimilar primero para poder desarrollar adecuadamente nuestra intuici√≥n en la materia. Recalcar, adem√°s, la importancia de la estimaci√≥n del error del modelo. En la pr√≥xima entrada explicar√© como funciona Kaggle en ese aspecto, con datos de entrenamiento y de test, con <em>leaderboard</em> p√∫blico y privado.</p>
<p>Cualquier comentario o sugerencia respecto a la entrada es bien recibida. Cr√≠ticas tambi√©n. Palos, no. Os leo.</p>
<h2>Referencias</h2>
<p>Kaggle.com, (2015). <em>Kaggle: The Home of Data Science</em>. [online] Available at: http://kaggle.com [Accessed 10 Jan. 2015].</p>
<p>Pedregosa, F., Varoquaux, G., Gramfort, A. et al. (2011). Scikit-learn: Machine Learning in Python. <em>Journal of Machine Learning Research</em>, 12, pp. 2825‚Äì2830.</p>
<p>Richert, W. and Coelho, L. (2013).¬†<em>Building Machine Learning Systems with Python</em>. Birmingham: Packt Publishing.</p>
<p>Schaul, T., Bayer, J., Wierstra, D.¬†et al.¬†(2010). PyBrain. <em>Journal of Machine Learning Research</em>, 11, pp. 743‚Äì746.</p>
        </div>

        <div class="meta">
            <div class="tags">
                    <a href="http://pybonacci.github.io/tag/dataset.html" class="tag">dataset</a>
                    <a href="http://pybonacci.github.io/tag/iris.html" class="tag">iris</a>
                    <a href="http://pybonacci.github.io/tag/kaggle.html" class="tag">kaggle</a>
                    <a href="http://pybonacci.github.io/tag/machine-learning.html" class="tag">machine learning</a>
                    <a href="http://pybonacci.github.io/tag/python.html" class="tag">python</a>
                    <a href="http://pybonacci.github.io/tag/scikit-learn.html" class="tag">scikit-learn</a>
            </div>
        </div>


    </article>
<div id="commento"></div>
</div>

<style type="text/css">
{
    max-width: 700px;
}

.text_cell .prompt {
    display: none;
}

div.cell {
    padding: 0;
}

div.text_cell_render {
    padding: 0;
}

div.prompt {
    font-size: 13px;
}

div.input_prompt {
    padding: .7em 0.2em;
}

div.output_prompt {
    padding: .4em .2em;
}

div.input_area {
    margin: .2em 0.4em;
    max-width: 580px;
}

table.dataframe {
    font-family: Arial, sans-serif;
    font-size: 13px;
    line-height: 20px;
}

table.dataframe th, td {
    padding: 4px;
    text-align: left;
}

pre code {
    background-color: inherit;
}</style>

        </div>

        <footer class="disclaimer">
          <div class="container-fluid">
            <p>
              ¬© 2012-2017 Pybonacci, licencia <a href="https://github.com/Pybonacci/pybonacci.github.io/blob/sources/LICENSE.md"> CC BY-SA 4.0 + MIT</a>
              salvo otra indicaci√≥n.
              <p>Contenido generado con <a href= "http://docs.getpelican.com/">Pelican</a>.</p>
            </p>
          </div>
        </footer>

    </body>
</html>